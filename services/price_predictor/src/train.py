import pandas as pd
from comet_ml import Experiment
from feature_engineering import FeatureEngineer
from pydantic import BaseModel, Field
from train_evaluation import Evaluator, Trainer

from tools.logging_config import logger
from tools.ohlc_data_reader import OhlcDataReader
from tools.settings import SupportedCoins, settings

from utils import log_best_model  # isort:skip
from utils import compare_models, load_models, log_model_results  # isort:skip


FEATURE_ENGINEER_CONFIG = "src/configs/config.yaml"
BASELINE_MODEL_CONFIG = "src/configs/baseline_config.yaml"
CHALLENGER_MODEL_CONFIG = "src/configs/challenger_config.yaml"


class TrainingConfig(BaseModel):
    """Configuration for model training pipeline."""

    feature_view_name: str = Field(..., description="Name of the feature view")
    feature_view_version: int = Field(
        ..., description="Version of feature view"
    )
    product_id: list[str] = Field(
        ..., description="List of product IDs to process"
    )
    last_n_days_to_fetch_from_store: int = Field(
        ..., description="Days of data to fetch"
    )
    last_n_days_to_test_model: int = Field(..., description="Days for testing")
    prediction_window_tick: int = Field(
        ..., description="Future prediction window"
    )
    test_size_ratio: float = Field(
        default=0.2, description="Ratio of test size for cross-validation"
    )
    feature_config_path: str = Field(
        default=FEATURE_ENGINEER_CONFIG,
        description="Path to feature engineering config",
    )
    baseline_config_path: str = Field(
        default=BASELINE_MODEL_CONFIG,
        description="Path to baseline model config",
    )
    challenger_config_path: str = Field(
        default=CHALLENGER_MODEL_CONFIG,
        description="Path to challenger model config",
    )


def setup_experiment(config: TrainingConfig) -> Experiment:
    """Initialize and configure CometML experiment.

    Args:
    ----
    config: Training configuration parameters

    """
    experiment = Experiment(
        api_key=settings.comet_ml.api_key,
        project_name=settings.comet_ml.project_name,
        workspace=settings.comet_ml.workspace,
    )
    experiment.log_parameters(config.model_dump())
    return experiment


def fetch_ohlc_data(
    config: TrainingConfig, experiment: Experiment
) -> pd.DataFrame:
    """Fetch OHLC data from feature store.

    Args:
    ----
    config: Training configuration parameters
    experiment: CometML experiment

    """
    ohlc_data_reader = OhlcDataReader(
        feature_view_name=config.feature_view_name,
        feature_view_version=config.feature_view_version,
    )
    ohlc_data = ohlc_data_reader.read_from_offline_store(
        product_id=config.product_id,
        last_n_days=config.last_n_days_to_fetch_from_store,
    )
    experiment.log_dataset_hash(ohlc_data)
    return ohlc_data


def temporal_train_test_split(
    ohlc_data: pd.DataFrame, last_n_days_to_test_model: int
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Split the data into train and test splits.

    Split is a temporal split. Last_n_days_to_test_model define
    the number of days for testing the model.

    Args:
    ----
    ohlc_data: The OHLC data to split.
    last_n_days_to_test_model: The number of days to test the model on.

    """
    max_data_in_dataset = ohlc_data["end_time"].max()
    cutoff_date = max_data_in_dataset - pd.Timedelta(
        days=last_n_days_to_test_model
    )
    train_df = ohlc_data[ohlc_data["end_time"] < cutoff_date]
    test_df = ohlc_data[ohlc_data["end_time"] >= cutoff_date]
    return train_df, test_df


def create_target_variable(
    ohlc_data: pd.DataFrame,
    prediction_window_tick: int,
) -> pd.DataFrame:
    """Create the target variable based on future close percentage changes.

    The target variable is generated by comparing the percentage change in the
    close price of the current tick with the close price of a future tick,
    defined by `prediction_window_tick`.

    Args:
    ----
    ohlc_data: The OHLC data to create the target variable for.
    prediction_window_tick: The number of ticks in the future to compare

    """
    ohlc_data = ohlc_data.copy()
    ohlc_data["pct_change"] = (
        ohlc_data.groupby("product_id")["close"].pct_change(
            periods=prediction_window_tick
        )
        * 100
    )
    ohlc_data.loc[:, "pct_change"].fillna(0, inplace=True)
    ohlc_data["target"] = ohlc_data.groupby("product_id")["pct_change"].shift(
        -prediction_window_tick
    )
    return ohlc_data.dropna(subset=["target"])


def main(config: TrainingConfig) -> None:
    """Run main pipeline to generate price predictions.

    The model follows these steps:
    1. Data validation and integrity checks
    2. Fetch OHLC data from feature store
    3. Create target variable
    4. Feature engineering
    5. Cross-validation training
    6. Model evaluation and selection
    7. Model persistence

    Args:
    ----
    config: Training configuration parameters

    """
    # Create experiment to log metadata to CometML
    experiment = setup_experiment(config)
    # Step 1 - Fetch OHLC data
    ohlc_data = fetch_ohlc_data(config, experiment)

    logger.info("Splitting data into train and test sets.")
    train_df, test_df = temporal_train_test_split(
        ohlc_data, config.last_n_days_to_test_model
    )
    experiment.log_metrics(
        {
            "n_rows_train": train_df.shape[0],
            "n_rows_test": test_df.shape[0],
        }
    )

    # Create target variable
    logger.info("Creating target variable for trainset.")
    train_df = create_target_variable(train_df, config.prediction_window_tick)
    logger.info("Creating target variable for testset.")
    test_df = create_target_variable(test_df, config.prediction_window_tick)
    logger.info(
        "--------Train / Test split ----------- \n"
        f"Train: {train_df["product_id"].count()}\n"
        f"Test: {test_df["product_id"].count()}\n"
    )

    # Prepare features
    X_train, X_test, y_train, y_test, X_train_features, X_test_features = (
        prepare_features(
            train_df, test_df, config.feature_config_path, experiment
        )
    )

    # Train and evaluate baseline models
    best_baseline, baseline_metrics = train_baseline_models(
        X_train, y_train, X_test, y_test, config, experiment
    )

    # Train and evaluate challenger models
    trained_challengers, best_challenger, challenger_metrics = (
        train_challenger_models(
            X_train_features,
            y_train,
            X_test_features,
            y_test,
            config,
            experiment,
        )
    )

    # Log best model
    log_best_model(
        model=trained_challengers[best_challenger],
        model_name=settings.comet_ml.model_name,
        best_baseline_metric=baseline_metrics[best_baseline]["MAPE"],
        best_challenger_metric=challenger_metrics[best_challenger]["MAPE"],
        experiment=experiment,
    )


def prepare_features(
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    feature_config_path: str,
    experiment: Experiment,
) -> tuple[
    pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, pd.DataFrame
]:
    """Prepare features for model training.

    Args:
    ----
    train_df: Training DataFrame
    test_df: Test DataFrame
    feature_config_path: Path to feature engineering config
    experiment: CometML experiment for logging

    """
    # Split into features and target
    X_train, y_train = train_df.drop("target", axis=1), train_df["target"]
    X_test, y_test = test_df.drop("target", axis=1), test_df["target"]

    # Apply feature engineering
    logger.info("Applying feature engineering")
    feature_engineering = FeatureEngineer(feature_config_path)
    X_train_features = feature_engineering.add_features(X_train)
    X_test_features = feature_engineering.add_features(X_test)

    # Log feature shapes
    experiment.log_metrics(
        {
            "X_train_shape_rows": X_train_features.shape[0],
            "X_train_shape_cols": X_train_features.shape[1],
            "X_test_shape_rows": X_test_features.shape[0],
            "X_test_shape_cols": X_test_features.shape[1],
        }
    )

    return X_train, X_test, y_train, y_test, X_train_features, X_test_features


def train_baseline_models(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    config: TrainingConfig,
    experiment: Experiment,
) -> tuple[str, dict[str, dict[str, float]]]:
    """Train baseline models.

    Args:
    ----
    X_train: Training features
    y_train: Training targets
    X_test: Test features
    y_test: Test targets
    config: Training configuration
    experiment: CometML experiment for logging

    """
    logger.info("Training baseline models")
    baseline_models = load_models(config.baseline_config_path)
    baseline_trainer = Trainer(baseline_models)
    trained_baselines = baseline_trainer.train_all_models(
        X_train,
        y_train,
        pct_change_train_mean=(
            X_train["pct_change"].mean() if "pct_change" in X_train else 0
        ),
    )
    evaluator = Evaluator(metrics=["MAE", "MAPE"])
    baseline_metrics_train = evaluator.evaluate_all_models(
        trained_baselines, X_train, y_train
    )
    logger.info(f"Train Baseline metrics: {baseline_metrics_train}")
    baseline_metrics_test = evaluator.evaluate_all_models(
        trained_baselines, X_test, y_test
    )
    logger.info(f"Test Baseline metrics: {baseline_metrics_test}")
    log_model_results("TEST", baseline_metrics_test, experiment)
    best_baseline = compare_models(baseline_metrics_test, "MAPE")
    logger.info(
        f"Best baseline by MAPE = {best_baseline} with "
        f"{baseline_metrics_test[best_baseline]}"
    )
    return best_baseline, baseline_metrics_test


def train_challenger_models(
    X_train_features: pd.DataFrame,
    y_train: pd.Series,
    X_test_features: pd.DataFrame,
    y_test: pd.Series,
    config: TrainingConfig,
    experiment: Experiment,
) -> tuple[dict, str, dict[str, dict[str, float]]]:
    """Train challenger models.

    Args:
    ----
    X_train_features: Training features
    y_train: Training targets
    X_test_features: Test features
    y_test: Test targets
    config: Training configuration
    experiment: CometML experiment for logging

    """
    logger.info("Training challenger models")
    challenger_models = load_models(config.challenger_config_path)
    challenger_trainer = Trainer(challenger_models)
    trained_challengers = challenger_trainer.train_all_models(
        X_train_features, y_train
    )
    evaluator = Evaluator(metrics=["MAE", "MAPE"])
    challenger_metrics_train = evaluator.evaluate_all_models(
        trained_challengers, X_train_features, y_train
    )
    logger.info(f"Train Challenger metrics: {challenger_metrics_train}")
    challenger_metrics_test = evaluator.evaluate_all_models(
        trained_challengers, X_test_features, y_test
    )
    logger.info(f"Test Challenger metrics: {challenger_metrics_test}")
    log_model_results("TEST", challenger_metrics_test, experiment)
    best_challenger = compare_models(challenger_metrics_test, "MAPE")
    logger.info(
        f"Best challenger by MAPE = {best_challenger} with "
        f"{challenger_metrics_test[best_challenger]}"
    )
    return trained_challengers, best_challenger, challenger_metrics_test


if __name__ == "__main__":
    config = TrainingConfig(
        feature_view_name="ohlc_feature_view",
        feature_view_version=1,
        product_id=[SupportedCoins.BTC_USD.value, SupportedCoins.ETH_USD.value],
        last_n_days_to_fetch_from_store=90,
        last_n_days_to_test_model=30,
        prediction_window_tick=1,
    )
    main(config)
